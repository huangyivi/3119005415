| 这个作业属于哪个课程 | <https://edu.cnblogs.com/campus/gdgy/InformationSecurity1912-Softwareengineering> |
| -------------------- | ------------------------------------------------------------ |
| 这个作业要求在哪里   | <https://edu.cnblogs.com/campus/gdgy/InformationSecurity1912-Softwareengineering/homework/1214> |
| 这个作业的目标       | 论文查重                                                     |


## Github地址
<https://github.com/huangyivi/3119005415>

## PSP表格
| ***\*PSP2.1\****                        | ***\*Personal Software Process Stages\**** | ***\*预估耗时（分钟）\**** | ***\*实际耗时（分钟）\**** |
| --------------------------------------- | ------------------------------------------ | -------------------------- | -------------------------- |
| Planning                                | 计划                                       | 30                         | 40                         |
| · Estimate                              | · 估计这个任务需要多少时间                 | 30                         | 30                         |
| Development                             | 开发                                       | 4320                       | 2880                       |
| · Analysis                              | · 需求分析 (包括学习新技术)                | 30                         | 40                         |
| · Design Spec                           | · 生成设计文档                             | 40                         | 40                         |
| · Design Review                         | · 设计复审                                 | 20                         | 20                         |
| · Coding Standard                       | · 代码规范 (为目前的开发制定合适的规范)    | 10                         | 10                         |
| · Design                                | · 具体设计                                 | 60                         | 60                         |
| · Coding                                | · 具体编码                                 | 240                        | 180                        |
| · Code Review                           | · 代码复审                                 | 30                         | 30                         |
| · Test                                  | · 测试（自我测试，修改代码，提交修改）     | 200                        | 200                        |
| Reporting                               | 报告                                       | 40                         | 50                         |
| · Test Repor                            | · 测试报告                                 | 20                         | 20                         |
| · Size Measurement                      | · 计算工作量                               | 10                         | 10                         |
| · Postmortem & Process Improvement Plan | · 事后总结, 并提出过程改进计划             | 20                         | 20                         |
|                                         | · 合计                                     | 5400                       | 3630                       |

## 依赖介绍

### jieba v0.42.1

- 支持三种分词模式：

> 精确模式，试图将句子最精确地切开，适合文本分析；
> 全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义；
> 搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。

- 支持繁体分词
- 支持自定义词典
- MIT 授权协议

### gensim v4.1.0

- 介绍：Gensim（generate similarity）是一个简单高效的自然语言处理Python库，用于抽取文档的语义主题（semantic topics）。Gensim的输入是原始的、无结构的数字文本（纯文本），内置的算法包括Word2Vec，FastText，潜在语义分析（Latent Semantic Analysis，LSA），潜在狄利克雷分布（Latent Dirichlet Allocation，LDA）等，通过计算训练语料中的统计共现模式自动发现文档的语义结构。这些算法都是非监督的，这意味着不需要人工输入——仅仅需要一组纯文本语料。一旦发现这些统计模式后，任何纯文本（句子、短语、单词）就能采用语义表示简洁地表达。

- 特点：

  > 1. Memory independence： 不需要一次性将整个训练语料读入内存，Gensim充分利用了Python内置的生成器（generator）和迭代器（iterator）用于流式数据处理，内存效率是Gensim设计目标之一。
  > 2. Memory sharing： 训练好的模型可以持久化到硬盘，和重载到内存。多个进程之间可以共享相同的数据，减少了内存消耗。
  > 3. 多种向量空间算法的高效实现： 包括Word2Vec，Doc2Vec，FastText，TF-IDF，LSA，LDA，随机映射等。
  > 4. 支持多种数据结构。
  > 5. 基于语义表示的文档相似度查询

## 主要实现
### 流程图
![image](https://img2020.cnblogs.com/blog/2317477/202109/2317477-20210914155342983-1977853752.png)

### TF算法

TF-IDF(term frequency–inverse document frequency)是一种用于信息检索与数据挖掘的常用加权技术，常用于挖掘文章中的关键词，而且算法简单高效，常被工业用于最开始的文本数据清洗。

TF-IDF有两层意思，一层是"词频"（Term Frequency，缩写为TF），另一层是"逆文档频率"（Inverse Document Frequency，缩写为IDF）。

**当有TF(词频)和IDF(逆文档频率)后，将这两个词相乘，就能得到一个词的TF-IDF的值。某个词在文章中的TF-IDF越大，那么一般而言这个词在这篇文章的重要性会越高，所以通过计算文章中各个词的TF-IDF，由大到小排序，排在最前面的几个词，就是该文章的关键词。**

第一步，计算词频：

![img](https://pic4.zhimg.com/80/v2-281a550de928afe343c055d06371cf77_720w.jpg)

考虑到文章有长短之分，为了便于不同文章的比较，进行"词频"标准化。

![img](https://pic2.zhimg.com/80/v2-393435b342546a2f1736d1d755adb1cd_720w.jpg)

第二步，计算逆文档频率：

这时，需要一个语料库（corpus），用来模拟语言的使用环境。

![img](https://pic2.zhimg.com/80/v2-1d5c436e04f497544d72fec6909a3fad_720w.jpg)

如果一个词越常见，那么分母就越大，逆文档频率就越小越接近0。分母之所以要加1，是为了避免分母为0（即所有文档都不包含该词）。log表示对得到的值取对数。

第三步，计算TF-IDF：

![img](https://pic3.zhimg.com/80/v2-5560a4b2efa3330021b8b2ef13a471fe_720w.jpg)

可以看到，TF-IDF与一个词在文档中的出现次数成正比，与该词在整个语言中的出现次数成反比。所以，自动提取关键词的算法就很清楚了，就是**计算出文档的每个词的TF-IDF值，然后按降序排列，取排在最前面的几个词。**



### 思路

1. 首先，对文章句子进行拆分，生成句子列表，然后对每个句子进行分词。
2. 调用gensim模块的corpora.Dictionary方法，对原文件进行词典分析，生成语料库。
3. 通过gensim模块的models.tf模型算法进行余弦相似度计算，并建立索引。
4. 对待检测文件进行遍历，对每个句子选出最大相似度计算权重。
5. 利用加权后的文章总权重除以文章总长度，得到结果。

### 关键函数
- 对文章进行分句
![image](https://img2020.cnblogs.com/blog/2317477/202109/2317477-20210914160741353-529401968.png)

- 对句子进行分词
![image](https://img2020.cnblogs.com/blog/2317477/202109/2317477-20210914160804696-780364270.png)

- 计算相似度
![image](https://img2020.cnblogs.com/blog/2317477/202109/2317477-20210914160817026-393100050.png)

## 异常处理
- 如果传入路径的目标文件不存在或者为空，则会抛出异常：
![image](https://img2020.cnblogs.com/blog/2317477/202109/2317477-20210918102325464-94793413.png)


## 单元测试

- 引入unitest模块，分别对五个待检测的文件进行论文查重。

- 部分代码：

  ![image](https://img2020.cnblogs.com/blog/2317477/202109/2317477-20210915152303995-1345257475.png)

- 运行结果如下：
  ![image](https://img2020.cnblogs.com/blog/2317477/202109/2317477-20210915152921486-1260301536.png)
  
## 性能分析
- 使用Pycharm性能分析工具的结果如下：
	![image](https://img2020.cnblogs.com/blog/2317477/202109/2317477-20210915152728099-1951101506.png)

## 代码覆盖率
- 使用Pycharm的代码覆盖率查看如下，其中77%的代码执行，剩余代码为异常处理：
	![image](https://img2020.cnblogs.com/blog/2317477/202109/2317477-20210918102138294-1093482074.png)


## 个人总结
- 由于之前没有接触过python代码，因此在上手语言的部分花了比较多的时间，后面了解了TF算法以及python相关的库，最后将本系统成功做出来了。
- 本项目让我了解到一个工程需要注意的点，特别是在写PSP表格的过程中，在完成项目的过程中根本没有考虑这么多工序，相信在以后的开发中会对我有很大的帮助。